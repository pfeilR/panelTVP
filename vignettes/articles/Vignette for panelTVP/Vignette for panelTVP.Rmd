---
title: A Vignette for the \texttt{R} Package \texttt{panelTVP}
author:
  - name: Roman Pfeiler
    email: roman.pfeiler@jku.at
  - name: Helga Wagner
    email: helga.wagner@jku.at
address:
  - code: Institute of Applied Statistics
    organization: Johannes Kepler University
    city: Linz
    country: Austria
abstract: |
  In this vignette, the \texttt{R} package \texttt{panelTVP} is introduced. The package allows for the estimation of flexible Bayesian regression models for  panel data. The model is flexible in the sense that both regression effects and random intercepts are allowed to vary over time. The use of hierarchical shrinkage priors prevents 
  overfitting and makes it possible to identify whether an effect is time-varying, time-invariant or zero. The response variable can either be Gaussian, binary or a (zero-inflated) count.
keywords: 
  - Bayesian statistics
  - Panel data
  - Time-varying parameter models
  - Shrinkage priors
  - Pólya-Gamma data augmentation
journal: "An awesome journal"
date: "`r Sys.Date()`"
linenumbers: false
numbersections: true
bibliography: mybibfile.bib
biblio-style: elsarticle-harv
classoption: a4paper, preprint, 3p, authoryear 
output: 
  rticles::elsevier_article:
    keep_tex: true
    citation_package: natbib
    latex_engine: lualatex
header-includes:
  - \newcommand{\pandocbounded}[1]{#1}
---

# Introduction

```{r, echo=FALSE}
# install_github("pfeilR/panelTVP")
library(panelTVP)
```

Panel data are commonly encountered in econometrics and the social sciences. When analyzing data from longitudinal studies, standard estimation techniques such as OLS (\textit{ordinary least squares}) are not appropriate as the repeated measurements of a subject are typically correlated. Therefore, models with random effects are used to account for the clustered data structure. 

\texttt{panelTVP} makes it possible to estimate regression models with random effects and additionally allows the parameters to vary over time. To avoid overfitting, hierarchical shrinkage priors are considered. The method implemented in \texttt{panelTVP} was inspired by Bitto \& Frühwirth-Schnatter (2019) who discussed the use of shrinkage priors in time-varying parameter models for the analysis of time series data. Their method has later been implemented in the \texttt{R} package \texttt{shrinkTVP} (Knaus, Bitto-Nemling, Cadonna, Frühwirth-Schnatter, 2021), which was partly used for creating the algorithm in \texttt{panelTVP}. 

The package supports Gaussian response data as well as binary (Probit and Logit link) and count outcomes. For analyzing count data, either a standard Negative Binomial or a Zero-Inflated Negative Binomial (ZINB) model can be fitted. Compared to a Poisson model, the use of the Negative Binomial distribution has the advantage that overdispersion can be handled more naturally through the overdispersion parameter of the Negative Binomial distribution.

This vignette is structured as follows: Section 2 introduces the reader to the methodology. This includes the model, the prior distributions and basic information of the Markov Chain Monte Carlo (MCMC) sampler that is used in \texttt{panelTVP}. Details of the sampler are not discussed in this vignette. Section 3 provides a small case study for Gaussian response data, whereas Section 4 and 5 contain case studies for binary and count data, respectively. Finally, Section 6 concludes this vignette and provides a brief summary of the content in the package.

# Methodology

## Shrinkage in time-varying parameter panel models

We consider a balanced panel $\text{y}_{it}$ with $t = 1,\dots,T$ observations for $i = 1,\dots,n$ subjects, where $\text{y}_{it}$ is either a Gaussian, binary or count response. The linear predictor of the model is given as 
\begin{equation}
\label{centered}
    \eta_{it} = \textbf{x}_{it}^\top \boldsymbol{\beta}_t + f_i\lambda_t,
\end{equation}
where $\textbf{x}_{it}$ is a $d \times 1$ vector of covariate values (including a $1$ for the intercept as the first value), $\boldsymbol{\beta}_t$ is a $d \times 1$ vector of regression effects at time $t,$ $\lambda_t$ is a time-specific factor loading and $f_i$ is a subject-specific factor score. The factor model with time-specific factor loadings in $\eta_{it}$ can be interpreted as a model with a time-varying random intercept. For reasons discussed below, we refer to Equation (\ref{centered}) as the linear predictor of the model in centered representation.

The goal is now to identify whether an effect is time-varying, time-invariant or zero. For this, a random walk specification of the effects is considered, i.e.,
\begin{equation*}
\begin{aligned}
    \boldsymbol{\beta}_t &= \boldsymbol{\beta}_{t-1} + \boldsymbol{\nu}^\beta_t, \quad &\boldsymbol{\nu}^\beta_t \sim \mathcal{N}(\textbf{0}, \textbf{Q}), \quad t= \{2,\dots,T\}, \\
    \lambda_t &= \lambda_{t-1} + \nu^\lambda_t, \quad &\nu^\lambda_t \sim \mathcal{N}(0, \psi^2), \quad t= \{2,\dots,T\},  \\    
\end{aligned}
\end{equation*}
where $\textbf{Q} = \text{diag}(\theta^2_1,\dots,\theta^2_d)$ and we assume that the process starts at $T=1$ with starting distributions
\begin{equation*}
    \begin{aligned}
        \boldsymbol{\beta}_1 &\sim \mathcal{N}(\boldsymbol{\beta}, c^\beta\textbf{Q}), \\
        \lambda_1 &\sim \mathcal{N}(\lambda, c^\lambda \psi^2).
    \end{aligned}
\end{equation*}
Alternatively, it is also possible to let the processes start at $T=0.$ Simulations have shown that results are very similar when starting at $T=0$ or $T=1$.

In order to place prior distributions on all model parameters, we exploit the non-centered parameterization that was used for state space models (Frühwirth-Schnatter and Wagner, 2010), where the time-varying parameters are expressed as 
\begin{equation*}
    \begin{aligned}
        \boldsymbol{\beta}_t &= \boldsymbol{\beta} + \boldsymbol{\Theta} \tilde{\boldsymbol{\beta}}_t, \\
        \lambda_t &= \lambda + \psi \tilde{\lambda}_t,
    \end{aligned}
\end{equation*}
where $\boldsymbol{\Theta} = \text{diag}(\theta_1,\dots,\theta_d).$ 

In the non-centered representation the random processes are defined as
\begin{equation*}
\begin{aligned}
    \tilde{\boldsymbol{\beta}}_t &= \tilde{\boldsymbol{\beta}}_{t-1} + \tilde{\boldsymbol{\nu}}^\beta_t, \quad \tilde{\boldsymbol{\nu}}^\beta_t \sim \mathcal{N}(\textbf{0}, \textbf{I}) \\
    \tilde{\lambda}_t &= \tilde{\lambda}_{t-1} + \tilde{\nu}^\lambda_t, \quad \tilde{\nu}^\lambda_t \sim \mathcal{N}(0,1)
\end{aligned}
\end{equation*}
with starting distributions $\tilde{\boldsymbol{\beta}}_0 \sim \mathcal{N}(\textbf{0},c^\beta \textbf{I})$ and $\tilde{\lambda}_0 \sim \mathcal{N}(0,c^\lambda)$ or $\tilde{\boldsymbol{\beta}}_1 \sim \mathcal{N}(\textbf{0},c^\beta \textbf{I})$ and $\tilde{\lambda}_1 \sim \mathcal{N}(0,c^\lambda)$ depending on the random walk specification. The linear predictor in non-centered representation is then given by
\begin{equation}
\label{non-centered}
    \eta_{it} = \textbf{x}_{it}^\top \boldsymbol{\beta} + \textbf{x}_{it}^\top \boldsymbol{\Theta} \tilde{\boldsymbol{\beta}}_t + f_i\lambda + f_i \psi \tilde{\lambda}_t.
\end{equation}

A major advantage when using the non-centered model representation is that the parameters of interest $\boldsymbol{\beta}, \boldsymbol{\Theta}, \lambda$ and $\psi$ are all regression coefficients and, thus, well-known prior distributions from regression analysis can be placed on those coefficients. As in Bitto and Frühwirth-Schnatter (2019) and Pfeiler and Wagner (2024), we consider Normal-Gamma shrinkage priors
\begin{equation*}
\begin{aligned}
        \theta_j|\xi_j^2 &\sim \mathcal{N}(0,\xi^2_j), \quad \xi_j^2|a^\xi, \kappa^\xi \sim \mathcal{G}\Bigl(a^\xi, \frac{a^\xi \kappa^\xi}{2}\Bigr), \; j=\{1,\dots,d\}, \\
        \beta_j|\tau_j^2 &\sim \mathcal{N}(0,\tau_j^2), \quad \tau_j^2|a^\tau, \kappa^\tau \sim \mathcal{G}\Bigl(a^\tau, \frac{a^\tau \kappa^\tau}{2}\Bigr),  \; j=\{1,\dots,d\}, \\
        \psi|\zeta^2 &\sim \mathcal{N}(0,\zeta^2), \quad \zeta^2|a^\zeta, \kappa^\zeta \sim \mathcal{G}\Bigl(a^\zeta, \frac{a^\zeta \kappa^\zeta}{2}\Bigr), \\
        \lambda|\phi^2 &\sim \mathcal{N}(0,\phi^2), \quad \phi^2|a^\phi, \kappa^\phi \sim \mathcal{G}\Bigl(a^\phi, \frac{a^\phi \kappa^\phi}{2}\Bigr),
\end{aligned}
\end{equation*}
where the scale parameters of the stochastic processes $\theta_1,\dots\theta_d,\psi$ have support on $\mathbb{R}$ and are therefore defined as the positive and negative square root of the variance parameters $\theta^2_1,\dots,\theta^2_d,\psi^2.$  

Hyperparameters of the model are either fixed or sampled using Metropolis-Hastings updates.

## Pólya-Gamma data augmentation

Bayesian inference for binary and count data is challenging as the posterior is in general not available in closed form and, thus, MCMC techniques are necessary. Within the machinery of MCMC, a Metropolis-Hastings (MH) algorithm based on an IWLS (\textit{iteratively weighted least squares}) proposal distribution (Gamerman, 1997) is one possibility. However, updating the parameters using MH is potentially costly due to low acceptance rates in high-dimensional parameters settings (Fahrmeir, Kneib, Lang and Marx, 2021). In their seminal paper, Albert and Chib (1993) discuss a data augmentation scheme for Probit regression based on the latent utility representation of binary choice models. For Logit models, their sampler cannot be used due to the non-Gaussianity of errors in the latent utility representation of the Logit model. However, Polson, Scott and Windle (2013) introduced an efficient data augmentation scheme for posterior inference in models with binomial likelihoods. Their method can be used for Bayesian inference in Logit and Negative Binomial regression models and is briefly outlined in the following as it is also used in the \texttt{panelTVP} package.

The basic idea is that conditional on a Pólya-Gamma distributed random variable, Bayesian inference in the Logit or Negative Binomial model is based on a Gaussian posterior. A random variable $\omega$ follows a Pólya-Gamma distribution with parameters $b > 0$ and $c \in \mathbb{R},$ if
\begin{equation*}
    \omega \overset{D}{=} \frac{1}{2\pi^2}\sum_{k=1}^\infty \frac{g_k}{(k-1/2)^2+c^2/(4\pi^2)},
\end{equation*}
where $g_k \sim \mathcal{G}(b,1)$ are independent Gamma random variables. Algorithms for sampling from the Pólya-Gamma distribution are implemented in the \texttt{R}-package \texttt{BayesLogit} (Polson, Scott and Windle, 2013).

Polson, Scott and Windle (2013) established the following integral identity
\begin{equation}
\label{Integral}
    \frac{(\exp(\eta))^a}{(1+\exp(\eta))^b} = 2 ^{-b}\exp(\eta\delta)\int_0^\infty \exp\Bigl(-\frac{\omega\eta^2}{2}\Bigr) p(\omega) \; d\omega,
\end{equation}
where $\eta$ is a linear predictor, $\delta = a-b/2$ and $\omega \sim \mathcal{PG}(b,0)$ follows a Pólya-Gamma distribution.

The likelihood contribution in the Logit panel model in centered parameterization (see Equation (\ref{centered})) is given by
\begin{equation*}
 p(\text{y}_{it}|\textbf{x}_{it},\boldsymbol{\beta}_t,f_i,\lambda_t,\omega_{it}) = \frac{(\exp(\eta_{it}))^{\text{y}_{it}}}{1+\exp(\eta_{it})} \propto  \exp\Bigl(-\frac{\omega_{it}}{2}\Bigl(\frac{\text{y}_{it}-1/2}{\omega_{it}}-\eta_{it}\Bigr)^2\Bigr),
\end{equation*}
which follows from Equation (\ref{Integral}) and by completing the square. Hence, conditional on the Pólya-Gamma variable $\omega,$ the coefficients in the Logit model can be sampled from a Gaussian posterior. Moreover, the distribution of $\omega$ - conditional on the parameters in the linear predictor $\eta$ - is a Pólya-Gamma distribution as well, which makes Gibbs sampling feasible. 

In order to perform inference in the Bayesian Negative Binomial model, we first consider a specific parameterization of the Negative Binomial distribution, where
\begin{equation*}
 p(\text{y}_{it}|\textbf{x}_{it},\boldsymbol{\beta}_t,f_i,\lambda_t) \propto \Bigl(1-\frac{\exp(\eta_{it})}{1+\exp(\eta_{it})}\Bigr)^r \Bigl(\frac{\exp(\eta_{it})}{1+\exp(\eta_{it})}\Bigr)^{\text{y}_{it}} = \frac{(\exp(\eta_{it}))^{\text{y}_{it}}}{(1+\exp(\eta_{it}))^{\text{y}_{it} + r}}
\end{equation*}
Using this parameterization, the conditional moments are then given by
\begin{equation*}
 \mathbb{E}(\text{y}_{it}|\textbf{x}_{it},\boldsymbol{\beta}_t,f_i,\lambda_t) \equiv \mu_{it} = r  \exp(\eta_{it}), \quad \mathbb{V}(\text{y}_{it}|\textbf{x}_{it},\boldsymbol{\beta}_t,f_i,\lambda_t) = r \exp(\eta_{it}) (1 + \exp(\eta_{it})).
\end{equation*}
After some algebra and by using the integral identity from Equation (\ref{Integral}), the likelihood contribution in the Negative Binomial panel data model can be expressed as
\begin{equation*}
 p(\text{y}_{it}|\textbf{x}_{it},\boldsymbol{\beta}_t,f_i,\lambda_t, \omega_{it}) \propto 
 \frac{(\exp(\eta_{it}))^{\text{y}_{it}}}{(1+\exp(\eta_{it}))^{\text{y}_{it} + r}} \propto \exp\Bigl(-\frac{\omega_{it}}{2}\Bigl(\frac{\text{y}_{it}-r}{2\omega_{it}} - \eta_{it}\Bigr)^2\Bigr).
\end{equation*}
Hence, inference in Bayesian Negative Binomial models is again based on a Gaussian posterior conditional on the Pólya-Gamma variables that are sampled in an additional data augmentation step (see Pillow and Scott (2012) for more details on Pólya-Gamma data augmentation in Negative Binomial models).

## Zero-Inflated Negative Binomial model

For modelling both zero-inflated and overdispersed count data, we assume that there are two latent classes of zeros to account for the excess zeros: structural and at-risk zeros. A structural zero belongs to an observation that is not at risk of experiencing the event, whereas an at-risk zero belongs to an observation that is at-risk of experiencing the event but has for some reason a zero recorded. It is therefore assumed that the outcome in the Zero-Inflated Negative Binomial model
$\text{y}_{it}$ is a realization of a mixture distribution with a point mass at zero (for the structural zeros) and a standard Negative Binomial count model for observations that are at-risk,
i.e., this includes both at-risk zeros and positive counts. The ZINB model can, thus, be stated as
\begin{equation*}
 \text{y}_{it}|r,\mu_{it},w_{it} \sim (1-\pi_{it}) \cdot \mathbb{I}_{(w_{it} = 0)} + \pi_{it} \cdot \mathcal{NB}(\mu_{it},r) \cdot \mathbb{I}_{(w_{it}=1)},
\end{equation*}
where ${w_{it}}$ is a latent at-risk indicator such that with probability ${1-\pi_{it}, w_{it} = 0}$ and with probability ${\pi_{it}, w_{it} = 1}$, and ${\mu_{it}}$ is the mean of the Negative Binomial distribution, as defined previously.

There are two separate linear predictors in the ZINB model. The first
linear predictor ${\eta_{it}^{\text{logit}}}$ is related to the zero-inflated
part of the model through
${\pi_{it} = \frac{\exp(\eta_{it}^{\text{logit}})}{1+\exp(\eta_{it}^{\text{logit}})},}$
whereas the second linear predictor ${\eta_{it}^{\text{nb}}}$ is the linear predictor of the Negative Binomial regression model. As the zero-inflation part is typically modelled with a Logit model, the Zero-Inflated Negative Binomial model contains both a Logit and a Negative Binomial regression model (for more details on Bayesian Zero-Inflated Negative Binomial regression see Neelon, 2019).

## MCMC sampling schemes

In the following, the general sampling steps for the various regression models in \texttt{panelTVP} are briefly outlined. Details on the sampling steps are omitted in this vignette. For the sampling steps, we define $\boldsymbol{\lambda} = (\lambda_1,\dots,\lambda_T)^\top,$ $\mathbf{f} = (f_1,\dots,f_n)^\top,$ $\boldsymbol{\omega}=(\omega_{11},\dots,\omega_{nT})^\top$ and $\textbf{X}_t,$ is an $(n\times d)$ regressor matrix with rows $\textbf{x}_{it}^\top.$

### Gaussian model

The Gaussian panel data model in centered representation is given by
\begin{equation*}
 \text{y}_{it} = \textbf{x}_{it}^\top \boldsymbol{\beta}_t + f_i\lambda_t + \varepsilon_{it}, \quad \varepsilon_{it} \sim \mathcal{N}(0,\sigma^2),
\end{equation*}
where $\sigma^2$ is the constant error variance. In the Gaussian model, posterior inference is easier than in the other models as a Pólya-Gamma data augmentation step is not required. However, the homoscedastic error variance must be sampled unlike in the other models. For this, we assume a  conjugate Inverse-Gamma prior for $\sigma^2$, i.e.,
\begin{equation*}
 \sigma^2|C_0 \sim \mathcal{G}^{-1}(c_0, C_0), \quad C_0 \sim \mathcal{G}(g_0,G_0),
\end{equation*}
where the parameters $c_0$, $g_0$ and $G_0$ are set to fixed values. 

Bayesian inference in the Gaussian panel data model then involves looping over the following steps:
\begin{enumerate}
    \item Sample the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T$ the factors $\mathbf{f},$ the loadings $\boldsymbol{\lambda}$ and the homoscedastic error variance $\sigma^2$ from the posterior of a Normal regression model with response $\text{y}_{it}-f_i\lambda_t.$
    \item Sample the factors $\mathbf{f}$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ the loadings $\boldsymbol{\lambda}$ and the homoscedastic error variance $\sigma^2$ from the posterior of a Normal regression model with response  $\text{y}_{it}-\textbf{x}_{it}^\top \boldsymbol{\beta}_t$.
    \item Sample the loadings $\boldsymbol{\lambda}$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ the factors $\mathbf{f}$ and the homoscedastic error variance $\sigma^2$ from the posterior of a Normal regression model with response  $\text{y}_{it}-\textbf{x}_{it}^\top \boldsymbol{\beta}_t.$
     \item Sample the homoscedastic error variance $\sigma^2$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ the factors $\mathbf{f}$ and the factor loadings $\boldsymbol{\lambda}$ from an Inverse-Gamma posterior.
\end{enumerate}

### Probit model

The Probit model can be represented as a latent utility model with standard Normal noise, i.e.,
\begin{equation*}
  \tilde{\text{y}}_{it} = \textbf{x}_{it}^\top \boldsymbol{\beta}_t + f_i\lambda_t + \varepsilon_{it}, \quad \varepsilon_{it} \sim \mathcal{N}(0,1),
\end{equation*}
where $\tilde{y}_{it}$ is the latent utility that is linked to the observed binary response via a threshold mechanism: The observed response is 1 when $\tilde{\text{y}}_{it} > 0$, and it is 0 when $\tilde{\text{y}}_{it} \le 0.$ The latent utility $\tilde{\text{y}}_{it}$ can easily be sampled from a truncated Normal distribution (Albert and Chib, 1993). Therefore, Bayesian inference in the Probit panel data model involves looping over the following steps:
\begin{enumerate}
   \item Sample the latent utility $\tilde{\text{y}}_{it}$ either from a positively truncated Normal distribution, when $\text{y}_{it} = 1,$ or from a negatively truncated Normal distribution, when $\text{y}_{it} = 0.$
    \item Sample the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T$ the factors $\mathbf{f},$ and the loadings $\boldsymbol{\lambda}$ from the posterior of a Normal regression model with response $\tilde{\text{y}}_{it}-f_i\lambda_t.$
    \item Sample the factors $\mathbf{f}$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ and the loadings $\boldsymbol{\lambda}$  from the posterior of a Normal regression model with response  $\tilde{\text{y}}_{it}-\textbf{x}_{it}^\top \boldsymbol{\beta}_t$.
    \item Sample the loadings $\boldsymbol{\lambda}$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ and the factors $\mathbf{f}$ from the posterior of a Normal regression model with response  $\tilde{\text{y}}_{it}-\textbf{x}_{it}^\top \boldsymbol{\beta}_t.$
\end{enumerate}

### Logit model

Bayesian inference in the Logit model requires a Pólya-Gamma data augmentation step. Hence, Gibbs sampling involves looping over the following steps:

\begin{enumerate}
    \item Sample $\omega_{it}|\textbf{x}_{it},\boldsymbol{\beta}_t,f_i,\lambda_t \sim \mathcal{PG}(1, \eta_{it})$ and define the new response as $\text{y}_{it}^* \equiv (\text{y}_{it}-1/2)/\omega_{it}\; \forall i,t$.
    \item Sample the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T$ the factors $\mathbf{f},$ the loadings $\boldsymbol{\lambda}$ and the weights $\boldsymbol{\omega}$ from the posterior of a Normal regression model with response $\text{y}_{it}^*-f_i\lambda_t.$
    \item Sample the factors $\mathbf{f}$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ the loadings $\boldsymbol{\lambda}$ and the weights $\boldsymbol{\omega}$ from the posterior of a Normal regression model with response  $\text{y}_{it}^*-\textbf{x}_{it}^\top \boldsymbol{\beta}_t$.
    \item Sample the loadings $\boldsymbol{\lambda}$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ the factors $\mathbf{f}$ and the weights $\boldsymbol{\omega}$ from the posterior of a Normal regression model with response  $\text{y}_{it}^*-\textbf{x}_{it}^\top \boldsymbol{\beta}_t.$
\end{enumerate}

### Negative Binomial model

Bayesian inference in the Negative Binomial model also requires a Pólya-Gamma data augmentation step and an additional step to sample the dispersion parameter of the Negative Binomial distribution. The dispersion parameter could be sampled by Metropolis-Hastings, but we achieved better results with univariate Slice sampling. In this MCMC technique, the parameter of interest is drawn uniformly from a region under the graph of its posterior distribution. The Slice sampler, thus, requires less tuning than the Metropolis-Hastings sampler. Moreover, the random walk behavior of the chain can be suppressed by adding an overrelaxation step (see Neal (2003) for further details on Slice sampling). Posterior inference in the Negative Binomial model then involves looping over the following steps:

\begin{enumerate}
    \item Sample $\omega_{it}|\textbf{x}_{it},\boldsymbol{\beta}_t,f_i,\lambda_t,\text{y}_{it}, r \sim \mathcal{PG}(\text{y}_{it}+r, \eta_{it})$ and define the new response as $\text{y}_{it}^* \equiv (\text{y}_{it}-r)/(2\omega_{it})\; \forall i,t$.
    \item Sample the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T$ the factors $\mathbf{f},$ the loadings $\boldsymbol{\lambda},$ the weights $\boldsymbol{\omega}$ and the dispersion parameter $r$ from the posterior of a Normal regression model with response $\text{y}_{it}^*-f_i\lambda_t.$
    \item Sample the factors $\mathbf{f}$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ the loadings $\boldsymbol{\lambda},$ the weights $\boldsymbol{\omega}$ and the dispersion parameter $r$ from the posterior of a Normal regression model with response  $\text{y}_{it}^*-\textbf{x}_{it}^\top \boldsymbol{\beta}_t$.
    \item Sample the loadings $\boldsymbol{\lambda}$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ the factors $\mathbf{f},$ the weights $\boldsymbol{\omega}$ and the dispersion parameter $r$ from the posterior of a Normal regression model with response  $\text{y}_{it}^*-\textbf{x}_{it}^\top \boldsymbol{\beta}_t.$
    \item Sample the dispersion parameter $r$ conditional on the covariates $\textbf{X}_1,\dots,\textbf{X}_T,$ the regression effects $\boldsymbol{\beta}_1,\dots,\boldsymbol{\beta}_T,$ the factors $\mathbf{f}$, the loadings $\boldsymbol{\lambda}$ and the weights $\boldsymbol{\omega}$ using univariate Slice sampling. 
\end{enumerate}

### Zero-Inflated Negative Binomial model

As the Zero-Inflated Negative Binomial model consists of two sub-models, i.e., a Logit model for the zero-inflation component and a Negative Binomial model for the count component, the MCMC sampler is basically a combination of the two sampling schemes for Logit and Negative Binomial regression with an additional step for sampling the latent at-risk indicators. As a consequence, we have two separate linear predictors with time-varying regression and random effects, i.e.,
\begin{equation*}
 \begin{aligned}
 \eta_{it}^{\text{logit}} &= (\textbf{x}^{\text{logit}}_{it})^\top \boldsymbol{\beta}_t^{\text{logit}} + f_i^{\text{logit}} \lambda_t^{\text{logit}}, \\
  \eta_{it}^{\text{nb}} &= (\textbf{x}^{\text{nb}}_{it})^\top \boldsymbol{\beta}_t^{\text{nb}} + f_i^{\text{nb}} \lambda_t^{\text{nb}}.
 \end{aligned}
\end{equation*}
The same set of covariates can be used for modelling both components of the Zero-Inflated Negative Binomial distribution, but it is not mandatory to include the same covariates in both predictors.

An MCMC scheme for sampling the parameters of a Bayesian Zero-Inflated Negative Binomial panel model then involves looping over the following steps:
\begin{enumerate}
    \item Sample the latent at-risk indicators ${w}_{11},\dots,{w}_{nT}$ conditional on all model parameters.
    \item Update the parameters in the predictor of the zero-inflation component $\eta_{it}^{\text{logit}}$ by using the data augmentation sampler of the Logit model.
    \item For all observations that are currently in the at-risk class, i.e. for which $w_{it}=1$, update the parameters in the predictor of the count component $\eta_{it}^{\text{nb}}$ and the dispersion parameter of the Negative Binomial distribution $r$ by using the data augmentation sampler of the Negative Binomial model as well as the Slice sampler, respectively.
\end{enumerate}
Note that the at-risk indicators are only updated for observations for which $\text{y}_{it} = 0$. For observations with a positive count, $\text{w}_{it} = 1$ in all iterations of the MCMC sampler.

# Basic structure of \texttt{panelTVP}

Time-varying parameter models for panel data of varying response distributions can be fitted by using the package's main function \texttt{panelTVP::panelTVP()}. The basic structure of the function is as follows:

```{r, eval = FALSE, highlight = FALSE}

panelTVP(formula = NULL,
         data = NULL,
         id = NULL,
         t = NULL,
         model = NULL,
         prior.reg,
         prior.var,
         prior.load,
         prior.reg_zinb.count,
         prior.load_zinb.count,
         prior.reg_zinb.inflation,
         prior.load_zinb.inflation,
         mcmc.opt,
         settings.NegBin,
         HPD.coverage,
         R.WAIC,
         random.effects,
         progress.bar)

```

## Essential arguments

The \texttt{formula} argument is used for specifying the model frame and works in the same way as the \texttt{formula} argument of \texttt{lm()} and \texttt{glm()} when fitting a model to either a Gaussian, binary or Negative Binomial response. For a Zero-Inflated Negative Binomial outcome, the formula was inspired by the function \texttt{pscl::zeroinfl()}, which is used for fitting frequentist Zero-Inflated Negative Binomial models to cross-sectional data.

The \texttt{data} argument expects a \texttt{data.frame} object that contains all the variables used in the \texttt{formula} argument.

The arguments \texttt{id} and \texttt{t} are each vectors that are typically variables in the \texttt{data} object. The former is a subject-indicator, whereas the latter is a time-indicator. It is mandatory to specify those parameters and covariates in the \texttt{formula} argument are not allowed to be named either \texttt{id} or \texttt{t} to avoid confusion.

The argument \texttt{model} defines the model that should be fitted to the data. This argument is a single character that is either \texttt{"Gaussian"} (for a Gaussian model), \texttt{"Probit"} (for a Probit model), \texttt{"Logit"} (for a Logit model), \texttt{"NegBin"} (for a Negative Binomial model) or \texttt{"ZINB"} (for a Zero-Inflated Negative Binomial model). The character string you pass to the \texttt{model} argument is not restricted to case sensitivity, i.e., it is possible to write \texttt{model = "Logit"}, \texttt{model = "loGiT"} etc. Moreover, you can use \texttt{model = "Normal"} as an alias for \texttt{model = "Gaussian"} to fit a Gaussian / Normal model.

Those four arguments constitute the bare minimum for fitting a time-varying parameter model to balanced panel data with \texttt{panelTVP()}, e.g., a Logit model can be fitted by using

```{r, eval=F, highlight=F}

panelTVP(formula = y ~ W1 + W2,
         data = logit.data,
         id = logit.data$id,
         t = logit.data$t,
         model = "Logit")

```

A Zero-Inflated Negative Binomial model with two linear predictors can be fitted by using

```{r, eval=F, highlight=F}

panelTVP(formula = y ~ W1.count + W2.count | W1.inflation,
         data = zinb.data,
         id = zinb.data$id,
         t = zinb.data$t,
         model = "ZINB")

```

Note that when you specify \texttt{model = "ZINB"}, then the \texttt{formula} argument consists of two parts: the count and the zero-inflation predictor. The two predictors are separated by the \texttt{|} sign. Moreover, it is possible to include the same set of covariates in both predictors. When all covariates in the dataset should be included in both predictors, then you can write

```{r, eval=F, highlight=F}

panelTVP(formula = y ~ . | .,
         data = zinb.data,
         id = zinb.data$id,
         t = zinb.data$t,
         model = "ZINB")

```

This will also ignore the indicator variables \texttt{id} and \texttt{t} in your dataset, so you do not need to worry about excluding them.

## Arguments for MCMC settings

The arguments \texttt{mcmc.opt} and \texttt{settings.NegBin} both deal with MCMC settings and are expected to be lists that contain additional parameters. Note that the elements in a list can easily be changed without the need to copy the entire list elements. To illustrate this, we consider the argument \texttt{mcmc.opt} that is used for setting the options of the MCMC sampler. Its default arguments are:

```{r, eval=F, highlight=F}

mcmc.opt(chain.length = 12000, burnin = 2000, thin = 10, asis = TRUE)

```

The argument \texttt{chain.length} is the total number of MCMC draws, while \texttt{burnin} defines the burn-in period (i.e., per default, the first 2000 draws are discarded) and \texttt{thin} defines the thinning factor (i.e., per default, every tenth draw is used). Finally, \texttt{asis = TRUE} means that the optional ASIS (\textit{ancillarity sufficiency interweaving strategy}) step is used during MCMC sampling, which can increase sampling efficiency. Based on our simulations, we always recommend using ASIS as its computational burden is low and the gain in sampling efficiency can be tremendous. For more details on ASIS see, e.g., Yu and Meng (2011), Kastner and Frühwirth-Schnatter (2014) or Bitto and Frühwirth-Schnatter (2019). 

Suppose that we want to change the burnin in period to a higher value - say 4000. Then we can modify the list argument \texttt{mcmc.opt} in the following way:

```{r, highlight=F}

args <- rlang::fn_fmls(panelTVP) # extracts arguments of panelTVP
args.mcmc <- as.list(args$mcmc.opt) # selects mcmc.opt and converts it into a list
utils::modifyList(args.mcmc, list(burnin = 4000)) # this gives the altered list

```

Alternatively, modification of elements in a list can also be done in a more condensed way:

```{r, highlight=F}
utils::modifyList(as.list(rlang::fn_fmls(panelTVP)$mcmc.opt),
                  list(burnin = 4000))
```

The argument \texttt{settings.NegBin} is a list that is only used when fitting a model to a count response variable. Its default structure is as follows:

```{r, eval=F, highlight=F}

settings.NegBin = list(alpha.r = 2, beta.r = 1, expansion.steps = 10,               
                       width = 1, p.overrelax = 0, accuracy.overrelax = 10)

```

The six arguments of \texttt{settings.NegBin} are used for Slice sampling of the dispersion parameter $r$ of the Negative Binomial distribution. The arguments \texttt{alpha.r} and \texttt{beta.r} are shape and rate parameter of the Gamma prior on $r$, respectively. The argument \texttt{expansion.steps} defines the number of steps used in the stepping-out phase of the Slice sampler, while the \texttt{width} parameter defines the length of the slice interval. Those four arguments are required to perform standard Slice sampling. The standard Slice sampler can be extended to an overrelaxed Slice sampler, where the additional overrelaxation phase aims at increasing sampling efficiency by suppressing the random walk behavior of the Markov Chain. The accuracy of this procedure is governed by the argument \texttt{accuracy.overrelax}, which is the tuning parameter for mid-point refinement of the overrelaxed Slice sampler. If the accuracy limit is too small then the drawn value might not be in the Slice region, whereas high accuracy values will increase the computational burden of the sampler. Finally, the argument \texttt{p.overrelax} is the probability of performing an overrelaxed step. Hence, if \texttt{p.overrelax = 0} then no overrelaxation will be done and instead standard Slice sampling will be used. When overrelaxation is desired, then we recommend relatively high values, e.g., \texttt{p.overrelax = 0.95}. For details on the Slice sampler see the seminal paper Neal (2003).
 
## Arguments for prior specification

The arguments of \texttt{panelTVP()} that start with \texttt{prior.} are used for the specification of prior distributions for the model parameters. Each argument is a list that consists of several arguments. For fitting Gaussian, binary or Negative Binomial models the arguments \texttt{prior.reg} and \texttt{prior.load} are relevant. The former argument contains prior settings for the parameters of the regression part of the model, whereas the latter contains the corresponding parameters for the factor part of the model. For Gaussian models \texttt{prior.var} is also important as this argument is related to the prior for the homoscedastic error variance. When fitting a Zero-Inflated Negative Binomial (ZINB) model, the arguments \texttt{prior.reg\_zinb.count}, \texttt{prior.load\_zinb.count}, \texttt{prior.reg\_zinb.inflation} and \texttt{prior.load\_zinb.inflation} are relevant. Those arguments are essentially duplicated versions of \texttt{prior.reg} and \texttt{prior.load}, but allow for different prior specifications for the count and zero-inflated component of the ZINB model, respectively. Details on the prior settings can be found by calling

```{r eval=F, highlight=F}

help(panelTVP)

```

However, the \texttt{type} argument is very important and should, thus, be discussed in this vignette as well. With this argument, users can select among priors. For the regression part, this argument is either \texttt{type = "ind"}, \texttt{type = "rw-t0"} or \texttt{type = "rw-t1"}. The first option places an independent Normal prior on the regression parameters, whereas the remaining two place random walk shrinkage priors on the regression parameters. The only difference between the shrinkage priors is that the random walk will start at $T=0$ when \texttt{"rw-t0"} is selected and will start at $T=1$ when \texttt{"rw-t1"} is used. For the priors on the factor loadings, \texttt{type = "cps"} is also supported. This will assume a compound-symmetric covariance structure and, thus, random effects will then be assumed  as time-invariant a priori.
 
## Additional arguments

There are four remaining arguments of the function \texttt{panelTVP}, which will be covered in this subsection. 

The first one is \texttt{HPD.coverage}, which is the coverage probability of Bayesian highest posterior density intervals for the parameters with a $95 \; \%$ coverage as default. 

The argument \texttt{random.effects} is a Boolean indicating whether you want to include a random effects / factor structure in your model to account for unobserved heterogeneity. Random effects are usually included in panel models and, thus, \texttt{random.effects = TRUE} is the default setting.

The argument \texttt{R.WAIC} relates to the computation of the Widely Applicable Information Criterion (WAIC), where the marginal WAIC is computed on the basis of \texttt{R.WAIC} replicated datasets, see, e.g., Quintero and Lesaffre (2018) for details on the replication method and a discussion on the conditional and marginal WAIC for longitudinal models. 

Finally, the argument \texttt{progress.bar} is a Boolean that is set to \texttt{FALSE} as default and, thus, the progress of the sampler will not be printed out on the \texttt{R}-console. Change it to \texttt{TRUE} to keep track of the sampling progress.

# Analyzing Gaussian response data

We now demonstrate the usage of the \texttt{panelTVP} package based on an application of a real-world dataset - the \texttt{Income} dataset, which is contained in the package and a subset of the \textit{National Longitudinal Longitudinal Survey of Youth 1997}. This survey started its first round in 1997, where most of the participants were children or young adults. As the goal of this illustration is to analyze income data, we do not focus on the first waves of the survey. Moreover, due to the availability of some variables we only focus on the last four waves of the study, i.e., survey rounds in the years 2015, 2017, 2019 and 2021.

In the following, the first ten observations are printed out for a quick overview.

```{r, highlight=F}

head(Income, 10)

```

The variable \texttt{Edu} is a factor with levels "no degree", "high school degree" and "college degree", which refers to the highest completed degree of the person. \texttt{Hours.Worked} is a factor with levels "worked less than 30 hours" and "worked at least 30 hours". The variable \texttt{Baseline.Age\_c} is the centered version  of the baseline age of the person in the year 2015, i.e., the first study period considered in this specific subset of the survey. The variable \texttt{t} contains information the time point, whereas \texttt{id} is a subject indicator. Note that there are gaps in the ID enumeration, which is due to the fact that some subjects were removed for various reasons when the raw data were prepared for the purpose of creating this specific case study.

The response variable of interest is gross family income on the log-scale (\texttt{Gross.Family.Income\_log}). A quick descriptive analysis gives insight into the distribution of this variable as a function of time.

```{r, highlight=F}
boxplot(Income$Gross.Family.Income_log ~ Income$Year,
        xlab = "", ylab = "log(Gross.Family.Income)",
        main = "Distribution of Income (log-scale) over Time")
```

The boxplots suggest that the income distribution differs by time point and, thus, some covariates may differ in their effects as well.

The marginal distribution of the response by time point is given by

```{r, highlight=F}
Y <- sort(unique(Income$Year))
par(mfrow = c(2,2))
for(i in 1:4) hist(Income[Income$t == i,]$Gross.Family.Income_log, main = Y[i],
                   xlab = "log(Gross.Family.Income)")
layout(1)
```

Despite the slightly left-skewed marginal distribution of the response, we fit a Gaussian model with time-varying parameters by executing the following code:

```{r, eval=F, highlight=F}
f.Gauss <- Gross.Family.Income_log ~ Sex + Baseline.Age_c + Ethnicity + Edu + Hours.Worked
Gauss.vignette <- panelTVP(formula = f.Gauss,
                           data = Income,
                           id = Income$id,
                           t = Income$t,
                           model = "Gaussian")
```

```{r, echo=F}
Gauss.vignette <- readRDS("C:/Users/AK119624/Desktop/Dissertation/Paper 1/Real Analysis/income_packageData/Gauss.vignette.Rdata")
```

Information on the duration of the MCMC sampler can be accessed via

```{r, highlight=F}

Gauss.vignette$runtime

``` 

The model comprises a lot of parameters, but the most important ones are the time-varying regression effects and factor loadings. The S3 functions \texttt{summary()} and \texttt{plot()} can be used to show those effects in a table and in a plot, respectively. 

When summarizing the results in a table, the user can order the results either by covariate or by time point, i.e.,

```{r}
summary(Gauss.vignette)
```

```{r}
summary(Gauss.vignette, by = "timepoint")
```

Effect plots can be easily obtained as

```{r myplot, out.width="0.7\\linewidth"}

plot(Gauss.vignette, nplots = 2)

```

The first effect plot is always the random effect or the factor loading, which is followed by the global intercept. Afterwards, the time-varying effects of covariates are given. Based on the results we observe an increase in the overall income level over time and a significant decrease for the women. Hence, the interviewed women have ceteris paribus a lower gross family income than men and this effect is more pronounced as they get older. The other effect estimates are shrunken to time-invariance. We see that people with a high school or college degree earn on average more money (or at least have a higher family income) than people with no completed degree. Furthermore, working at least 30 hours yields to an increase in the gross family income compared to people who work less than 30 hours.

Aside from the inspection of the time-varying parameters, Bayesian inference with MCMC requires proper evaluation of the generated Markov Chains. The remaining MCMC draws after applying burnin and thinning for all parameters are stored in the matrix \texttt{mcmc}, i.e.,

```{r highlight=F}

head(Gauss.vignette$mcmc, 4)

```

The first columns are the sampled draws of the time-varying regression effects and, thus, the most interesting parameters. The first covariate is usually the global intercept. When using a random walk shrinkage prior, you also get the "fixed" effects or starting values of the random walks, which are in this case \texttt{beta1}$,\dots,$\texttt{beta8}, as well as the scale parameters of the random walks, which are in this case \texttt{theta1}$,\dots,$\texttt{theta8}. The next parameters are the hyperparameters of the shrinkage priors, which are in this case \texttt{tau21}$,\dots,$\texttt{tau28}, \texttt{xi21}$,\dots,$\texttt{xi28}, \texttt{a.tau}, \texttt{a.xi}, \texttt{kappa.tau}, \texttt{kappa.xi}. As we have fitted a Normal regression model, the homoscedastic error variance \texttt{sigma2} is also contained in the matrix. For a count regression model, the dispersion parameter of the Negative Binomial distribution will be included instead. The remaining parameters then relate to the factor loadings, but are otherwise similar to the corresponding parameters for the regression part of the model.

The chains can then be analyzed to assess convergence to the stationary distributions. For visual diagnostics, traceplots and plots of the ACFs are useful, e.g.,

```{r highlight=F}

par(mfrow = c(2,2))
plot(Gauss.vignette$mcmc[,"beta_t11"], type = "l", 
     main = "Trace of beta_t11 (Intercept at t = 1)", xlab = "", ylab = "")
acf(Gauss.vignette$mcmc[,"beta_t11"],
    main = "ACF of beta_t11 (Intercept at t = 1)")
plot(Gauss.vignette$mcmc[,"lambda_t1"], type = "l", 
     main = "Trace of lambda_t1 (Loading at t = 1)", xlab = "", ylab = "")
acf(Gauss.vignette$mcmc[,"lambda_t1"],
    main = "ACF of lambda_t1 (Loading at t = 1)")
layout(1)

```

A quantity for summarizing the information in a correlated sample in a single number is the effective sample size (ESS), which is defined as

\begin{equation*}
 \text{ESS} = \frac{M}{1+2\sum_{s=1}^\infty \rho_s}, 
\end{equation*}
where $M$ is the length of the chain (after burn-in and thinning) and $\rho_s$ is the autocorrelation of the chain at lag $s$. The ESS can be interpreted as the number of independent draws that is equivalent to $M$ MCMC draws. This parameter-specific quantity can be computed as 

```{r highlight=F}

LaplacesDemon::ESS(Gauss.vignette$mcmc[,"lambda_t1"])

```

As the length of the Markov Chain is $M = 1000$, we conclude that enough information was used for estimating the factor loading at the first time point.

The variables are coded in the order of their inclusion in the \texttt{formula} argument. For \texttt{summary()} and \texttt{plot()} the actual variable labels are used. However, this is not the case for, e.g., the \texttt{mcmc} object. For this, the object \texttt{variable.codes} was created, which maps the variable labels to their codes, i.e.,

```{r highlight=F}

Gauss.vignette$variable.codes

```

Finally, it should be mentioned how samples from the posterior predictive distribution can be obtained for the models fitted with \texttt{panelTVP()}. The object \texttt{posterior.predictive} contains $M$ draws from the posterior predictive distribution of the fitted values, i.e., the values of the response variable that were used for training the model. The object is a matrix where each row corresponds to an observation in the training data and each column to a draw of the Markov Chain (after burn-in and thinning).

```{r highlight=F}

dim(Gauss.vignette$posterior.predictive)

```

The posterior predictive distribution of the training data can be used for posterior predictive checks (PPC). Functions to perform PPC are, e.g., available in the \texttt{bayesplot} package. One way of assessing the fit of a Bayesian model is to superimpose the density of the original data with the $M$ densities of the posterior predictive distribution.

```{r highlight=F}

bayesplot::ppc_dens_overlay(y = Income$Gross.Family.Income_log,
                            yrep = t(Gauss.vignette$posterior.predictive))

```

Although the predicted densities are close to the density of the training data, we see that the pronounced peak at a log-income value of around $11.5$ is not fully captured. Moreover, information on the small peak located at the right tail of the distribution is not contained in the posterior predictive distributions. 

It is also possible to make predictions for new data using the S3 function \texttt{predict()}. This function has the following structure:

```{r eval = F, highlight=F}

predict(object, X.new, timepoint, coverage = 0.95,
        pop.pred = FALSE, n.replicates = 100, ...)

```

The argument \texttt{object} is an object that is returned by \texttt{panelTVP()}. The argument \texttt{X.new} is a \texttt{matrix} or a \texttt{data.frame} object, which contains the new data. Predictions are made for a specific time point that is specified in the argument \texttt{timepoint}. In addition to point predictions, Bayesian HPD prediction intervals are obtained as well with coverage probability that can be defined by using the argument \texttt{coverage}. The argument \texttt{pop.pred} is a Boolean and indicates whether the posterior predictive distribution of new data should be based on the population level or if random effects / factors should be included as well. If \texttt{pop.pred = TRUE}, then the subject-specific factors are integrated out as for new data subject-specific information is not available. For marginalization, \texttt{n.replicates} samples are drawn, where the latent factors are simulated from the prior. Therefore, the accuracy of marginalization increases as \texttt{n.replicates} increases - at the cost of increased computational complexity.

In the following, the prediction task is briefly demonstrated. Suppose we want to make predictions for a specific time point for two new subjects for our income data. For this, we first set up the new design matrix and then run the \texttt{predict()} function:

```{r highlight=F}

names.orig <- colnames(Gauss.vignette$data$X)

X.new <- data.frame(Intercept = c(1,1),
                    Sex = c(0,1),
                    Baseline.Age_c = c(12,16),
                    EthnicityHispanic = c(1,0),
                    EthnicityMixed = c(0,0),
                    highschool = c(1,0),
                    college = c(0,1),
                    Hours.Worked = c(0,1))
colnames(X.new) <- names.orig

pp <- predict(object = Gauss.vignette,
              X.new = X.new,
              timepoint = 4)

```

The object \texttt{pp} is a list consisting of two matrices. The matrix \texttt{predictive.summary} contains summary statistics derived from the predictive distribution, i.e., 

```{r highlight=F}

pp$predictive.summary

```

The matrix \texttt{predictive.distribution} contains the posterior predictive distribution and is of dimension $N_{new} \times M$, where $N_{new}$ is the number of new data points.

```{r highlight=F}

par(mfrow = c(2,1))

hist(pp$predictive.distribution[1,], 
     main = "Predictive Distribution of First New Subject",
     xlab = "log(Gross.Family.Income)",
     xlim = c(10.8,12.2),
     breaks = 25)

hist(pp$predictive.distribution[2,], 
     main = "Predictive Distribution of Second New Subject",
     xlab = "log(Gross.Family.Income)",
     xlim = c(10.8,12.2),
     breaks = 25)

layout(1)

```

# Analyzing binary response data

In this section we focus on the analysis of binary outcomes using the Logit model. A corresponding Probit regression model can easily be set up by changing the \texttt{model} argument accordingly and, thus, will not be discussed here.

For demonstration, we will use the Marijuana dataset that is contained in the \texttt{panelTVP} package. This dataset is a pre-processed subset of the \textit{National Longitudinal Longitudinal Survey of Youth 1997} and contains data of the first seven waves of this study. The response variable is \texttt{Used.Mari.Since.DLI} and indicates if the person has consumed Marijuana since the last interview date. A one means that the person has consumed the drug. For the first wave the question was, if the person has ever consumed Marijuana in his/her life. The goal is to explain the probability of drug consumption as a function of covariates and to assess whether effects of covariates are subject to temporal change.

To get an overview of the data, the first 10 rows are given in the following:

```{r, highlight=F}
head(Marijuana, 10)
```

The variable \texttt{Bullied.Before.12} indicates whether the person was a victim of bullying before the age of 12. The variable \texttt{Residence} gives information on the location where the person lives in and is either \texttt{Urban} or \texttt{Rural}. Finally, the variable \texttt{Baseline.Age\_c} is a centered version of the person's baseline age, i.e., the age in the first survey round in 1997.

A Bayesian Logit model with time-varying parameters and shrinkage priors on the effects can be estimated as follows:

```{r, eval=F, highlight=F}
f.logit <- Used.Mari.Since.DLI ~ Sex + Baseline.Age_c + Residence + Bullied.Before.12 + Ethnicity
logit.vignette <- panelTVP(formula = f.logit,
                           data = Marijuana,
                           id = Marijuana$id,
                           t = Marijuana$t,
                           model = "Logit")
```

```{r, echo=F}

logit.vignette <- readRDS("C:/Users/AK119624/Desktop/Dissertation/Paper 1/Real Analysis/marihuana_packageData/mari.res.Rdata")

```

The total runtime of the sampler was considerably longer than for the Gaussian model. This is due to the fact that the Logit model requires sampling of Pólya-Gamma weights in each step of the sampler.

```{r, highlight=F}

logit.vignette$runtime

```

The estimated effects can again be summarized in a table

```{r, highlight=F}

summary(logit.vignette)

``` 

or displayed graphically 

```{r}

plot(logit.vignette, nplots = 2)

```

Based on the estimated effects, we see an increase in the probability of Marijuana consumption over time - although the probability is, in general, quite low. The effect of baseline age decreases almost linearly over time and is close to zero in the second half of the study period. People who live in urban areas are more likely to consume Marijuana than those who live in rural areas. Moreover, this effect increases after the third time point. If a person was bullied before the age of 12, he or she has a higher probability of consuming Marijuana. This effect remains constant over the study period. The ethnicity also influences the consumption behavior and women are less likely to consume Marijuana than men. Finally, the factor loading increases linearly until the fifth panel wave and afterwards decreases linearly. Hence, the degree of between-subject heterogeneity varies over time.

# Analyzing synthetic zero-inflated and overdispersed count response data

In this section we describe how zero-inflated and overdispersed count data can be analyzed with the \texttt{panelTVP()} function. We consider synthetic data that we simulate using the function \texttt{sim\_panelTVP()} and compare our shrinkage prior approach to an unstructured approach, where independent Normal priors are placed on the regression effects and the factor loadings. 

The function \texttt{sim\_panelTVP} has the following structure:

```{r eval = F, highlight=F}

sim_panelTVP(n, Tmax, model,
             beta = NULL, theta = NULL,
             lambda = NULL, psi = NULL,
             beta_zinb.count = NULL, theta_zinb.count = NULL,
             lambda_zinb.count = NULL, psi_zinb.count = NULL,
             beta_zinb.inflation = NULL, theta_zinb.inflation = NULL,
             lambda_zinb.inflation = NULL, psi_zinb.inflation = NULL, 
             r = NULL, sigma2 = NULL)

```

Argument \texttt{n} is the number of subjects, whereas \texttt{Tmax} is the total number of repeated measurements or time points. Both parameters expect numeric scalars. The argument \texttt{model} defines the model from which data are simulated. This argument expects one of the following character strings \texttt{"Gaussian"}, \texttt{"Probit"}, \texttt{"Logit"}, \texttt{"NegBin"}, \texttt{"ZINB"}. No aliases were given so the user is not allowed to deviate from these naming conventions. 

The regression effects and the factor loadings are then simulated from the first-order random walks that were already discussed in the theoretical chapter of this vignette. The parameter \texttt{beta} is the vector of "fixed effects", whereas \texttt{theta} is the vector of scale parameters for  the random walks of the regression effects. \texttt{lambda} and \texttt{psi} are both scalars and the starting value as well as the scale parameter of the random walk for the factor loading, respectively. As there are two linear predictors in the Zero-Inflated Negative Binomial model, there are also two sets of random walk parameters. The arguments ending at \texttt{inflation} are the parameters for the Logit component, whereas the arguments  ending at \texttt{count} are the parameters  for the Negative Binomial component.

Finally, the arguments \texttt{r} and \texttt{sigma2} refer to the dispersion parameter of the Negative Binomial distribution and the homoscedastic error variance of the Normal distribution, respectively. 

In the following code chunck, data from a Zero-Inflated Negative Binomial panel model are simulated and then analyzed with a random walk shrinkage prior and an independence prior, respectively.

```{r eval = F, highlight=F}

set.seed(21)

# generation of synthetic data
sim.zinb <- sim_panelTVP(n = 1000, Tmax = 4, model = "ZINB",
                         beta_zinb.count = c(0.5,0,-0.2),
                         theta_zinb.count = c(0.1,0,0),
                         lambda_zinb.count = 0.3,
                         psi_zinb.count = 0,
                         beta_zinb.inflation = c(0.9,0.1,0),
                         theta_zinb.inflation = c(0.05,0,0),
                         lambda_zinb.inflation = 0.8,
                         psi_zinb.inflation = 0.1,
                         r = 2)

# analysis with shrinkage prior
shrink.zinb <- panelTVP(y ~ W1.nb + W2.nb | W1.logit + W2.logit,
                        data = sim.zinb$observed,
                        t = sim.zinb$observed$t,
                        id = sim.zinb$observed$id,
                        model = "ZINB")

# modifying settings in preparation for independence prior
prior.reg_zinb.count <-
  utils::modifyList(as.list(rlang::fn_fmls(panelTVP)$prior.reg_zinb.count),
                  list(type = "ind"))

prior.load_zinb.count <-
  utils::modifyList(as.list(rlang::fn_fmls(panelTVP)$prior.load_zinb.count),
                  list(type = "ind"))

prior.reg_zinb.inflation <-
  utils::modifyList(as.list(rlang::fn_fmls(panelTVP)$prior.reg_zinb.inflation),
                  list(type = "ind"))

prior.load_zinb.inflation <-
  utils::modifyList(as.list(rlang::fn_fmls(panelTVP)$prior.load_zinb.inflation),
                  list(type = "ind"))

# analysis with independence prior
ind.zinb <- panelTVP(y ~ W1.nb + W2.nb | W1.logit + W2.logit,
                     data = sim.zinb$observed,
                     t = sim.zinb$observed$t,
                     id = sim.zinb$observed$id,
                     prior.reg_zinb.count = prior.reg_zinb.count,
                     prior.load_zinb.count = prior.load_zinb.count,
                     prior.reg_zinb.inflation = prior.reg_zinb.inflation,
                     prior.load_zinb.inflation = prior.load_zinb.inflation,
                     model = "ZINB")

```

```{r, echo=F}

sim.zinb <- readRDS("sim.zinb.Rdata")
shrink.zinb <- readRDS("shrink.zinb.Rdata")
ind.zinb <- readRDS("ind.zinb.Rdata")

```

# Conclusion

```{r, highlight=F}



```

























